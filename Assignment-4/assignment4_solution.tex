\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}

\title{CSDS 313/413: Introduction to Data Analysis \\ Assignment 4: Clustering and Dimensionality Reduction \\ Solutions}
\author{Wiam Skakri}
\date{\today}

\begin{document}

\maketitle

\section{Task 1: Clustering and Dimensionality Reduction}

\subsection{Part A: Principal Component Analysis}

\subsubsection{Question 1: Cumulative Variance Explained by Principal Components}

\paragraph{Methodology}
We applied Principal Component Analysis (PCA) to the congressional votes dataset (\texttt{p1\_congress\_1984\_votes.csv}), which contains voting records of 435 U.S. House of Representatives members on 16 key issues in 1984. 


PCA was performed using \texttt{sklearn.decomposition.PCA} to identify the principal components that capture the maximum variance in the voting patterns.

\paragraph{Results}

Figure \ref{fig:pca_variance} shows the cumulative variance explained as a function of the number of principal components $k$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{pca_cumulative_variance.png}
    \caption{Cumulative variance explained by top $k$ principal components. The red dashed line indicates 90\% variance threshold, and the green dashed line indicates 95\% variance threshold.}
    \label{fig:pca_variance}
\end{figure}

\paragraph{Key Observations}
\begin{itemize}
    \item The first principal component (PC1) explains approximately 47\% of the total variance
    \item The first two principal components together explain approximately 56\% of the variance
    \item To reach 90\% cumulative variance, approximately \textbf{10 principal components} are required
    \item To reach 95\% cumulative variance, approximately \textbf{12 principal components} are required
\end{itemize}

\textbf{Recommendation: 10-12 principal components are sufficient to summarize the data.}

\paragraph{Interpretation}
The PCA results suggest that while there is some correlation among the 16 votes (otherwise we would need all 16 components), the voting issues are sufficiently diverse that 10-12 dimensions are needed to adequately represent the voting patterns. This could indicate that the votes span multiple policy domains (economic, social, foreign policy, etc.) that are not perfectly aligned along a single ideological axis.

\subsubsection{Question 2: Projection onto First 3 Principal Components}

\paragraph{Methodology}
We projected the 435 congress members onto the first 3 principal components and created scatter plots for three PC pairs: (PC1-PC2), (PC1-PC3), and (PC2-PC3). Each point represents a congress member, colored by party affiliation (Democrats in blue, Republicans in red).

\paragraph{Variance Explained by First 3 Components}
\begin{itemize}
    \item PC1: 47.40\% of total variance
    \item PC2: 8.84\% of total variance
    \item PC3: 7.07\% of total variance

\end{itemize}

\paragraph{Results}

Figure \ref{fig:pca_scatter} shows the three scatter plot pairs with party affiliation colors.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{pca_party_scatter_plots.png}
    \caption{Scatter plots of congress members projected onto first 3 principal components, colored by party affiliation. Left: PC1 vs PC2, Middle: PC1 vs PC3, Right: PC2 vs PC3.}
    \label{fig:pca_scatter}
\end{figure}

\paragraph{Quantitative Separation Analysis}

To objectively compare the separation quality of each PC pair, we calculated separation scores based on the ratio of between-party distance to within-party variance:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{PC Pair} & \textbf{Centroid Distance} & \textbf{Avg Within-Party Variance} & \textbf{Separation Score} \\
\hline
PC1-PC2 & 4.350 & 1.749 & 3.289 \\
PC1-PC3 & 4.324 & 1.632 & \textbf{3.385} \\
PC2-PC3 & 0.672 & 1.064 & 0.651 \\
\hline
\end{tabular}
\caption{Separation metrics for each principal component pair. Higher separation score indicates better party separation.}
\end{table}





\textbf{PC1-PC3 provides the best separation between parties (separation score: 3.385)}, followed closely by PC1-PC2 (3.289). PC2-PC3 shows poor separation (0.651).


\textbf{Yes, congress members with the same party affiliation show clear clustering patterns.}

\textbf{Evidence:}
\begin{itemize}
    \item \textbf{Visual clustering}: In both PC1-PC2 and PC1-PC3 plots, Democrats (blue) cluster on the right side, while Republicans (red) cluster on the left side
    \item \textbf{Clear separation along PC1}: The primary axis of variation (PC1) strongly separates the two parties with minimal overlap in the center region

\end{itemize}

\subsection{Part B: Clustering Analysis}

\paragraph{Methodology}

We applied unsupervised clustering to group the 435 congress members into 2 clusters based solely on their voting patterns, without using party affiliation information.

\paragraph{Clustering Algorithm: K-Means}

We chose the K-Means clustering algorithm with the following specifications:

\begin{itemize}
    \item \textbf{Algorithm}: K-Means clustering
    \item \textbf{Number of clusters (k)}: 2
    \item \textbf{Distance metric}: Euclidean distance
    \item \textbf{Random state}: 42 (for reproducibility)
\end{itemize}

\textbf{How K-Means Works:}
\begin{enumerate}
    \item Initialize 2 cluster centers using the k-means++ strategy
    \item Assign each congress member to the nearest cluster center
    \item Update cluster centers to be the mean (centroid) of all assigned members
    \item Repeat steps 2-3 until convergence (cluster assignments no longer change)
\end{enumerate}

\textbf{Distance Function:}

The Euclidean distance in 16-dimensional vote space:
\begin{equation}
d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{16} (x_i - y_i)^2}
\end{equation}

where $\mathbf{x}$ and $\mathbf{y}$ are the voting vectors of two congress members across the 16 issues.

\paragraph{Visualization}

Figure \ref{fig:clustering} shows the clustering results visualized on the first two principal components (PC1-PC2), which explain 56.24\% of the total variance.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{clustering_vs_party.png}
    \caption{Comparison of K-Means clustering results (left) vs actual party affiliations (right). Left: Unsupervised clustering result with Cluster 0 (purple) and Cluster 1 (orange), cluster centers marked with red X. Right: Ground truth party affiliations with Democrats (blue) and Republicans (red).}
    \label{fig:clustering}
\end{figure}

\paragraph{Answer: Are the Groups Visually Separated?}

\textbf{Yes, the two clusters are well-separated in the PC1-PC2 space.}

\textbf{Quantitative evidence:}
\begin{itemize}
    \item \textbf{Distance between cluster centers}: 4.844 (in PC space)
    \item \textbf{Average within-cluster spread}: 1.081
    \item \textbf{Separation ratio}: 4.480
\end{itemize}

The separation ratio of 4.48 (well above 2.0) indicates that the clusters are clearly separated with minimal overlap. The cluster centers (marked with red X in the left plot) are positioned far apart relative to the spread of points within each cluster.

\paragraph{Answer: Agreement with Party Affiliations}

\textbf{The clustering shows strong agreement (88.3\%) with actual party affiliations.}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Party} & \textbf{Cluster 0} & \textbf{Cluster 1} & \textbf{Total} \\
\hline
Democrat & 8 & \textbf{160} & 168 \\
Republican & \textbf{224} & 43 & 267 \\
\hline
\textbf{Total} & 232 & 203 & 435 \\
\hline
\end{tabular}
\caption{Confusion matrix comparing clustering results with party affiliations. Bold numbers indicate correct cluster assignments.}
\end{table}

\textbf{Key findings:}
\begin{itemize}
    \item \textbf{Overall accuracy}: 88.3\% (384 out of 435 correctly clustered)
    \item \textbf{Cluster 0 (purple)} predominantly contains Republicans: 224/232 = 96.6\%
    \item \textbf{Cluster 1 (orange)} predominantly contains Democrats: 160/203 = 78.8\%
    \item \textbf{Democrats}: 95.2\% correctly clustered (160/168)
    \item \textbf{Republicans}: 83.9\% correctly clustered (224/267)

\end{itemize}

\subsubsection{Statistical Significance: Permutation Test}

\paragraph{Methodology}

To assess whether the clustering structure we found is statistically significant (rather than occurring by chance), we performed a permutation test with 1,000 iterations.

\paragraph{Clustering Quality Score:}

We used the \textbf{silhouette score} as our quality metric:
\begin{itemize}
    \item Measures how well-separated and compact clusters are
    \item Range: -1 to 1, where higher values indicate better clustering
    \item Calculated without using party labels (purely unsupervised metric)
    \item Formula: $s = \frac{b - a}{\max(a, b)}$ where $a$ = mean intra-cluster distance, $b$ = mean nearest-cluster distance
\end{itemize}

\paragraph{Permutation Procedure}

\begin{enumerate}
    \item \textbf{Compute original score}: Apply K-means to real data, calculate silhouette score
    \item \textbf{Generate null distribution}: For each of 1,000 permutations:
    \begin{itemize}
        \item Randomly shuffle each congress member's votes across the 16 issues
        \item This destroys voting patterns while preserving vote distributions
        \item Apply K-means clustering to permuted data
        \item Calculate silhouette score
    \end{itemize}
    \item \textbf{Compare distributions}: Calculate p-value as the proportion of permuted scores $\geq$ original score
\end{enumerate}

\textbf{Null Hypothesis ($H_0$)}: The voting data has no inherent clustering structure; any observed clusters are due to random chance.

\textbf{Alternative Hypothesis ($H_1$)}: The voting data contains real structure that produces meaningful clusters.

\paragraph{Results}

Figure \ref{fig:permutation} shows the distribution of clustering scores under the null hypothesis (permuted data) compared to the original score.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{permutation_test_results.png}
    \caption{Permutation test results. Left: Histogram of silhouette scores from 1,000 permuted datasets (blue) compared to the original score (red dashed line). Right: Cumulative distribution function showing the original score at the 100th percentile.}
    \label{fig:permutation}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Original silhouette score & 0.3536 \\
Mean permuted score & 0.0622 \\
Standard deviation (permuted) & 0.0016 \\
Difference (original - mean permuted) & 0.2914 \\
Z-score & 182.49 \\
\hline
\textbf{P-value} & \textbf{$<$ 0.001} \\

\hline
\end{tabular}
\caption{Permutation test statistics comparing original clustering to null distribution.}
\end{table}

\paragraph{Answer: Is the Clustering Statistically Significant?}

\textbf{Yes, the clustering is highly statistically significant (p $<$ 0.001).}

\textbf{Evidence:}
\begin{itemize}
    \item \textbf{P-value $<$ 0.001}: Out of 1,000 permutations, zero achieved a score as high as the original (p = 0.0000)
    \item \textbf{Extreme Z-score (182.49)}: The original score is more than 182 standard deviations above the mean of random data

    \item \textbf{100th percentile}: The original score exceeds 100\% of permuted scores
    \item \textbf{Clear visual separation}: The histogram shows complete separation between null distribution (centered at 0.06) and original score (0.35)
\end{itemize}

\textbf{Conclusion}: We reject the null hypothesis and conclude that the two-cluster structure in 1984 congressional voting data represents a statistically significant and substantively meaningful division that corresponds to party affiliation.

\subsection{Part C: Clustering Comparison Analysis}

\subsubsection{Task 1: Quantifying Agreement with Mutual Information}

\paragraph{Methodology}

To quantify the agreement between cluster membership and party affiliation, we use \textbf{Normalized Mutual Information (NMI)}.

\paragraph{Normalized Mutual Information}

NMI measures how much knowing one variable tells us about another, normalized to a 0-1 scale:

$$NMI(X;Y) = \frac{MI(X;Y)}{\sqrt{H(X) \cdot H(Y)}}$$

where $MI$ is mutual information and $H$ is entropy.

\begin{itemize}
    \item Range: $[0, 1]$ where 0 = no relationship, 1 = perfect agreement
    \item Measures reduction in uncertainty about party when cluster is known
    \item Higher values indicate stronger association
\end{itemize}

\paragraph{Results for Clustering on All 16 Votes}

\textbf{NMI = 0.5097}

\textbf{Interpretation}: NMI = 0.51 indicates strong agreement between clustering and party affiliation. Knowing which cluster a congress member belongs to substantially reduces uncertainty about their party affiliation.

\subsubsection{Task 2: Comparison - Principal Components vs All Votes}

\paragraph{Methodology}

We compared two clustering approaches:
\begin{enumerate}
    \item \textbf{All 16 Votes}: K-means clustering on the full 16-dimensional vote space
    \item \textbf{First 2 PCs}: K-means clustering on the 2-dimensional principal component space (PC1-PC2)
\end{enumerate}

Both used the same K-means parameters (k=2, random state=42, k-means++ initialization) for fair comparison.

\paragraph{Results}

Figure \ref{fig:part_c} shows the comparison between the two clustering approaches.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{part_c_comparison.png}
    \caption{Comparison of clustering approaches. Top left: Clustering on all 16 votes (NMI = 0.5097). Top right: Clustering on first 2 PCs (NMI = 0.4851). Bottom left: Actual party affiliations. Bottom right: Agreement metrics comparison.}
    \label{fig:part_c}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Approach} & \textbf{NMI} & \textbf{Difference} \\
\hline
All 16 Votes & 0.5097 & - \\
First 2 PCs & 0.4851 & -0.0246 \\
\hline
\end{tabular}
\caption{NMI comparison between clustering approaches. All 16 votes achieves higher agreement with party affiliations.}
\end{table}

\paragraph{Answer: Which Clustering Agrees More with Party Affiliations?}

\textbf{Clustering on all 16 votes agrees more with party affiliations (NMI = 0.5097) compared to clustering on first 2 PCs (NMI = 0.4851).}


\textbf{Information retention}: The first 2 principal components capture only 56.24\% of the total variance, meaning 43.76\% of information is discarded. While PC1 strongly correlates with party (as shown in Part A), the missing dimensions contain additional party-discriminating information.

\textbf{Multiple policy dimensions}: Party differences span multiple policy domains (economic, social, foreign policy). While PC1-PC2 captures the dominant axes of variation, the full 16-dimensional space represents these nuances more completely.





\paragraph{Practical Implications}

The modest difference (0.025 in NMI) between approaches suggests:
\begin{enumerate}
    \item PC1-PC2 captures the \textit{majority} of party-relevant information
    \item The additional 14 dimensions provide incremental but meaningful improvement
    \item For exploratory analysis and visualization, PC1-PC2 is sufficient
    \item For maximizing classification accuracy, using all features is preferable
\end{enumerate}



\section{Task 2: Predictive Modeling}

\subsection{Can Your Model Taste Good Wine?}



\subsection{Part A: Label Design and Discretization}

\subsubsection{Discretization Strategy}

To convert the regression problem (predicting quality scores 0-10) into a binary classification task, I discretized the quality scores into two classes using the following criterion:

\begin{itemize}
    \item \textbf{Good Wine}: Quality score $> 5$
    \item \textbf{Bad Wine}: Quality score $\leq 5$
\end{itemize}

This threshold was applied consistently to both red and white wine datasets to ensure models trained on one wine type can be meaningfully evaluated on the other during cross-domain testing.

\subsubsection{Original Quality Score Distributions}

Before discretization, I analyzed the original quality score distributions in both datasets. Figure \ref{fig:task2_label_design} (top row) shows the original quality distributions.

\paragraph{Key Observations:}
\begin{itemize}
    \item \textbf{Red Wine}: Quality scores range from 3 to 8, with a mean of 5.64. The distribution is approximately normal, centered around quality scores 5 and 6 (681 and 638 samples respectively).
    \item \textbf{White Wine}: Quality scores range from 3 to 9, with a mean of 5.88. The distribution is also approximately normal, with the highest frequency at quality 6 (2,198 samples) and 5 (1,457 samples).
    \item \textbf{Comparison}: White wines have slightly higher average quality scores than red wines, and a wider quality range (3-9 vs. 3-8).
\end{itemize}

\subsubsection{Class Distribution After Discretization}

After applying the threshold of 5, the resulting class distributions are shown in Figure \ref{fig:task2_label_design} (bottom row):

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Wine Type} & \textbf{Good Count} & \textbf{Good \%} & \textbf{Bad Count} & \textbf{Bad \%} \\
\hline
Red Wine & 855 & 53.5\% & 744 & 46.5\% \\
White Wine & 3,258 & 66.5\% & 1,640 & 33.5\% \\
\hline
\end{tabular}
\caption{Class distribution after discretization using threshold $>$ 5.}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{task2_part_a_label_design.png}
    \caption{Task 2 Part A: Wine quality label design and discretization. \textbf{Top row}: Original quality score distributions for red wine (left), white wine (middle), and combined comparison (right), with the discretization threshold marked at quality = 5. \textbf{Bottom row}: Class distributions after discretization for red wine (left), white wine (middle), and comparison (right).}
    \label{fig:task2_label_design}
\end{figure}

\subsubsection{Rationale for Label Design}

I discretized wine quality using a threshold of 5 (quality $>$ 5 = Good, $\leq$ 5 = Bad) for the following reasons:

\textbf{Interpretability}: This threshold separates above-average wines (quality 6-9) from average and below-average wines (quality 3-5), which aligns with the intuitive notion of ``good'' wine based on the 0-10 quality scale. Wines with quality scores above 5 represent those that received favorable ratings from wine tasters.

\textbf{Class Balance}: This approach yields relatively balanced classes. Red wine: 53.5\% good, 46.5\% bad; White wine: 66.5\% good, 33.5\% bad. This balance helps prevent models from being biased toward the majority class and ensures both classes have sufficient representation for learning meaningful patterns.

\textbf{Consistent Application}: The threshold is applied identically to both red and white wine datasets, ensuring models trained on one type can be meaningfully evaluated on the other during cross-domain testing in Part B.

\textbf{Sufficient Samples}: Both classes have adequate sample sizes in each dataset for reliable model training and evaluation. Even the smaller class (red wine ``bad'') has 744 samples, providing sufficient data for robust learning.

\subsection{Part B: Model Training and Evaluation}

In this part, I trained two classification models with different complexity levels and evaluated their performance both within the same wine type (in-domain) and across different wine types (cross-domain).

\subsubsection{Model Selection}

I selected two classification models that differ in complexity and learning capacity:

\paragraph{Model 1: Logistic Regression (Simple Baseline)}

Logistic Regression is a linear classifier that models the probability of a sample belonging to a class using a logistic function. It serves as a simple, interpretable baseline.

\textbf{Parameters:}
\begin{itemize}
    \item Solver: L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)
    \item Maximum iterations: 1000
    \item Regularization: L2 (default)
\end{itemize}

\paragraph{Model 2: Neural Network (Complex Model)}

A Multi-Layer Perceptron (MLP) neural network with two hidden layers, capable of learning complex non-linear patterns.

\textbf{Architecture:}
\begin{itemize}
    \item Input layer: 11 features (physicochemical attributes)
    \item Hidden layer 1: 64 neurons with ReLU activation
    \item Hidden layer 2: 32 neurons with ReLU activation
    \item Output layer: 2 classes (Good/Bad wine)
\end{itemize}

\textbf{Training Parameters:}
\begin{itemize}
    \item Optimizer: Adam (adaptive learning rate)
    \item Batch size: 32
    \item Maximum epochs: 500
    \item Early stopping: Enabled (patience = 20 iterations)
    \item Validation split: 10\% of training data
    \item L2 regularization ($\alpha$ = 0.0001)
\end{itemize}

\subsubsection{Data Preparation}

\paragraph{Train/Test Split}

For each wine type (red and white), I split the data into training and testing sets using an 80/20 split with stratification to preserve class balance:

\begin{itemize}
    \item \textbf{Red Wine}: 1,279 training samples, 320 test samples
    \item \textbf{White Wine (before downsampling)}: 3,918 training samples, 980 test samples
\end{itemize}

\paragraph{Downsampling White Wine Training Set}

To ensure fair comparison between models trained on red and white wines, I downsampled the white wine training set to match the red wine training set size ($\sim$1,280 samples). Downsampling was performed using stratified sampling to maintain the class distribution (Good: 66.5\%, Bad: 33.5\%).

\textbf{Final Training Set Sizes:}
\begin{itemize}
    \item Red Wine: 1,279 samples
    \item White Wine (after downsampling): 1,278 samples
\end{itemize}

\paragraph{Feature Standardization}

All 11 physicochemical features were standardized (mean = 0, standard deviation = 1) using separate StandardScaler instances for red and white wine training sets. This ensures that features with different scales contribute equally to model training and prevents numerical instability in gradient-based optimization.

\subsubsection{Evaluation Strategy}

Each model was evaluated under four conditions to assess both in-domain performance and cross-domain generalization:

\paragraph{In-Domain Testing}
Models trained and tested on the same wine type:
\begin{itemize}
    \item \textbf{Red $\rightarrow$ Red}: Train on red wine, test on red wine
    \item \textbf{White $\rightarrow$ White}: Train on white wine, test on white wine
\end{itemize}

\paragraph{Cross-Domain Testing}
Models trained on one wine type and tested on the other:
\begin{itemize}
    \item \textbf{Red $\rightarrow$ White}: Train on red wine, test on white wine
    \item \textbf{White $\rightarrow$ Red}: Train on white wine, test on red wine
\end{itemize}

\paragraph{Performance Metrics}

For each evaluation scenario, I reported three metrics:
\begin{itemize}
    \item \textbf{Accuracy}: Overall proportion of correct predictions
    \item \textbf{Precision}: Proportion of predicted ``Good'' wines that are actually good
    \item \textbf{Recall}: Proportion of actual ``Good'' wines correctly identified
\end{itemize}

\subsubsection{Results}

Table \ref{tab:task2_partb_results} summarizes the performance of both models across all evaluation conditions.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Train On} & \textbf{Test On} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\
\hline
\multicolumn{6}{|c|}{\textbf{In-Domain Testing}} \\
\hline
Logistic Regression & Red & Red & 74.1\% & 76.8\% & 73.7\% \\
Neural Network & Red & Red & 70.3\% & 77.9\% & 62.0\% \\
\hline
Logistic Regression & White & White & 74.1\% & 76.5\% & 88.0\% \\
Neural Network & White & White & \textbf{75.4\%} & 77.5\% & 88.8\% \\
\hline
\multicolumn{6}{|c|}{\textbf{Cross-Domain Testing}} \\
\hline
Logistic Regression & Red & White & \textbf{64.9\%} & 83.6\% & 58.7\% \\
Neural Network & Red & White & 63.9\% & 82.5\% & 58.0\% \\
\hline
Logistic Regression & White & Red & \textbf{63.8\%} & 92.3\% & 35.1\% \\
Neural Network & White & Red & 55.6\% & 85.4\% & 20.5\% \\
\hline
\end{tabular}
\caption{Performance comparison of Logistic Regression and Neural Network across in-domain and cross-domain testing scenarios. Bold values indicate the best performance for each test condition.}
\label{tab:task2_partb_results}
\end{table}



\subsection{Part C: Analysis and Interpretation}

In this part, I interpret and compare the model results from Part B, analyzing performance consistency, generalization capability, and factors affecting cross-domain transfer.

\subsubsection{Visualization}

Figure \ref{fig:task2_partc_analysis} presents a comprehensive analysis of model performance across all evaluation scenarios, including performance comparison, degradation analysis, precision-recall trade-offs, consistency metrics, and a performance heatmap.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{task2_part_c_analysis.png}
    \caption{Task 2 Part C: Comprehensive model performance analysis. \textbf{Top}: Overall accuracy comparison across scenarios with in-domain/cross-domain distinction. \textbf{Middle Left}: Cross-domain performance degradation. \textbf{Middle Right}: Precision-recall trade-off analysis. \textbf{Bottom Left}: Model consistency with variance. \textbf{Bottom Right}: Performance metrics heatmap.}
    \label{fig:task2_partc_analysis}
\end{figure}

\subsubsection{Question 1: In-Domain Consistency and Overfitting}

\textbf{Which model demonstrated more consistent in-domain performance? Did you observe any signs of overfitting?}

Logistic Regression demonstrated superior consistency across in-domain settings, achieving exactly 74.1\% accuracy on both red and white wines (standard deviation = 0.0\%). In contrast, Neural Network showed more variability (mean = 72.9\%, std = 2.5\%), performing worse on red wine (70.3\%) but slightly better on white wine (75.4\%). This variance suggests the Neural Network is sensitive to dataset-specific characteristics, a sign of overfitting. The Neural Network's poor cross-domain performance (especially White$\rightarrow$Red at 55.6\%) compared to its in-domain performance confirms overfitting to wine-type-specific patterns rather than learning generalizable quality indicators.

\subsubsection{Question 2: Cross-Domain Generalization}

\textbf{Which model generalized better across wine types? How much did performance degrade?}

Logistic Regression generalized significantly better across wine types, maintaining a mean cross-domain accuracy of 64.3\% (Red$\rightarrow$White: 64.9\%, White$\rightarrow$Red: 63.8\%) with minimal variance (std = 0.6\%). Neural Network achieved only 59.8\% mean cross-domain accuracy with high variance (std = 4.1\%). Performance degradation: Logistic Regression experienced consistent drops of 9.2\% and 10.3\%, while Neural Network showed asymmetric degradation---6.4\% for Red$\rightarrow$White but a severe 19.8\% drop for White$\rightarrow$Red. The Neural Network's failure to generalize from white to red wine (recall = 20.5\%) demonstrates its over-reliance on wine-type-specific features.

\subsubsection{Question 3: Factors Explaining Cross-Domain Differences}

\textbf{What factors might explain the performance differences observed during cross-domain testing?}

Three primary factors explain the cross-domain performance differences: \textbf{(1) Feature distribution shift}: Red and white wines have different physicochemical profiles (different acidity levels, sulfur dioxide concentrations, alcohol content). Models trained on one distribution struggle when feature values fall outside their training range. \textbf{(2) Model complexity and overfitting}: The Neural Network's higher capacity allowed it to memorize wine-type-specific patterns rather than learning universal quality indicators, while Logistic Regression's simplicity enforced learning of more generalizable linear relationships. \textbf{(3) Label imbalance}: White wines have 66.5\% ``good'' wines versus red's 53.5\%, causing models to learn different decision boundaries that transfer poorly across domains.

\end{document}
